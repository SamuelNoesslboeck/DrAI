import utils.rpWebClient as rpWebClient
import raspberryPi.pdars as pdars
from utils.chains import *
import utils.les as les
from flask import Flask
import numpy as np
import json
import cv2

#Setting up the AI-Chain
#Setting up the Camera Client

AI_CHAIN = Chain( document = True, verbose = True )
AI_CHAIN_CONFIG = json.load( open( "./model/config.json" ) )
CLIENT = rpWebClient.RPClient()

#Ip Adresse of the Camera Server
#Will be changed throught the user input when starting the server
IP = "127.0.0.1"

app = Flask( __name__ )

@app.route( "/points" )
def getImage():
    global IP #, AI_CHAIN

    #Get the images from the raspberry pi
    try:
        #Get first image
        data = CLIENT.getImage()

        # Check if the request was successfull
        # If not return the error to the client

        if data[ "error" ] == "None":
            img1, configData = data[ "img" ], data[ "config" ] 
        else:
            return { "points": [], "error": data[ "error" ] }
        
        #Move the Servos of the plattform
        input( "Move Servos and press enter:" )
        #requests.request( method = "post", url = f"http://{IP}:5000/moveServos" )

        #Get second image after moving the servos
        data = CLIENT.getImage()

        # Check if the request was successfull
        # If not return the error to the client

        if data[ "error" ] == "None":
            img2, configData  = data[ "img" ], data[ "config" ] 
        else:
            return { "points": [], "error": data[ "error" ] }

    except Exception as e:
        print( e )
        return { "points": [], "error": "Error with the Raspberry Pi Connection\n" }

    #Store the config data from the raspberry pi
    #throught the storage the pdars software has accass to it too
    json.dump( configData, open( "./raspberrypi/config.json", "w" ) ) 

    pdars.SETTINGS = configData

    #Run the pdars software
    try:
        data = pdars.process( img1, img2, configData )
        mergedImg = data[ "img" ]

        #Rotate the image to get it in the right perspective for the stable diffusion algorithm
        mergedImg = cv2.rotate( mergedImg, cv2.ROTATE_90_CLOCKWISE )
        mergedImg = np.where( mergedImg == 255, 0, 255 )
        
        cv2.imwrite( AI_CHAIN_CONFIG[ "original-image-path" ], mergedImg )

    except Exception as e:
        print( e )
        return { "points": [], "error": "Error with the pdars Software\n" }
    
    
    #Run the AI-Chain
    #try:
    #    AI_CHAIN.run()
    #except Exception as e:
    #    return { "points": [], "error": "Error with the AI-Chain\n" + e }


    #Convert the image which was generated by the ai to points
    try:
        points = les.GetPointsFromImage( AI_CHAIN_CONFIG[ "original-image-path" ], "./test.png", penSize = configData[ "penSize" ] )
        points = les.linesToRealWorld( points, data[ "offsets" ], configData )

    except Exception as e:
        print( e )
        return { "points": [], "error": "Error with the LES-Software\n" }

    return { "points": points, "error": None }


if __name__ == "__main__":
    print( "\n" * 3 )
    print( "--------------------------------------------" )
    print( "||                                        ||" )
    print( "||      ----    ----      --     -----    ||" )
    print( "||      |    |  |   |     /  \\     |      ||" )
    print( "||      |    |  |---     /----\\    |      ||" )
    print( "||      -----   |   \\   /      \\ -----    ||" )
    print( "||                                        ||" )
    print( "--------------------------------------------" )
    print( "        Humans interacting with AI      " )
    print( "\n" * 3 )
    print( ">>> Enter the ip of the raspberry pi controller" )
    print( ">>> Example: 127.0.0.1" )
    IP = input( )

    app.run( host = "0.0.0.0", port = 40325 )